import httpx
from bs4 import BeautifulSoup
from datetime import datetime

BASE_URL = "https://college.snation.kz"
JOURNAL_LIST_URL = f"{BASE_URL}/kz/tko/control/journals"
LOAD_URL_TEMPLATE = JOURNAL_LIST_URL + "/{journal_id}/load-table?year_month={month}%2F{year}"


async def get_journal_with_cookie(cookie: str):
    """ÐŸÐ°Ñ€ÑÐ¸Ñ‚ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ð¸Ð· HTML Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¶ÑƒÑ€Ð½Ð°Ð»Ð°"""
    now = datetime.now()
    headers = {
        "Cookie": cookie,
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/129.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "X-Requested-With": "XMLHttpRequest",
        "Referer": JOURNAL_LIST_URL,
    }

    async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as client:
        # 1ï¸âƒ£ Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð¾Ð²
        resp = await client.get(JOURNAL_LIST_URL, headers=headers)
        if resp.status_code != 200:
            return f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð¾Ð²: {resp.status_code}"

        # 2ï¸âƒ£ Ð˜Ñ‰ÐµÐ¼ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ñ‹
        soup = BeautifulSoup(resp.text, "html.parser")
        subject_links = []
        for a in soup.find_all("a", href=True):
            if "/kz/tko/control/journals/" in a["href"]:
                subject_name = a.get_text(strip=True)
                journal_id = a["href"].split("/")[-1]
                if journal_id.isdigit():
                    subject_links.append((subject_name, journal_id))

        if not subject_links:
            return "âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ñ‹. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒ cookie."

        # 3ï¸âƒ£ Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
        all_results = [f"ðŸ“˜ *ÐžÑ†ÐµÐ½ÐºÐ¸ Ð·Ð° {now.month:02d}/{now.year}:*"]
        for subject, journal_id in subject_links:
            load_url = LOAD_URL_TEMPLATE.format(journal_id=journal_id, month=now.month, year=now.year)
            try:
                r = await client.get(load_url, headers=headers)
                if r.status_code != 200:
                    all_results.append(f"{subject}: âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° {r.status_code}")
                    continue

                grades = extract_grades_from_html(r.text)
                if grades:
                    all_results.append(f"{subject}: {grades}")
                else:
                    all_results.append(f"{subject}: âš ï¸ ÐžÑ†ÐµÐ½Ð¾Ðº Ð½ÐµÑ‚")

            except Exception as e:
                all_results.append(f"{subject}: âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° ({e})")

        return "\n".join(all_results)


def extract_grades_from_html(html: str) -> str:
    """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð¸Ð· HTML (div.sc-journal__table--cell-value)"""
    soup = BeautifulSoup(html, "html.parser")
    divs = soup.find_all("div", class_="sc-journal__table--cell-value")
    grades = [div.get_text(strip=True) for div in divs if div.get_text(strip=True).isdigit()]
    return ", ".join(grades) if grades else ""
