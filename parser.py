import httpx
from bs4 import BeautifulSoup

BASE_URL = "https://college.snation.kz"
JOURNAL_LIST_URL = f"{BASE_URL}/kz/tko/control/journals"


async def get_journal_with_cookie(cookie: str):
    """ĞŸĞ°Ñ€ÑĞ¸Ñ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ñƒ"""
    headers = {
        "Cookie": cookie,
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/129.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": BASE_URL,
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

    async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as client:
        # 1ï¸âƒ£ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ²
        list_resp = await client.get(JOURNAL_LIST_URL, headers=headers)
        if list_resp.status_code != 200:
            print(f"[DEBUG] ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ ÑĞ¿Ğ¸ÑĞºĞ° Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ğ¾Ğ²: {list_resp.status_code}")
            return None

        soup = BeautifulSoup(list_resp.text, "html.parser")
        subject_links = []

        # Ğ˜Ñ‰ĞµĞ¼ Ğ²ÑĞµ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ñ‹
        for a in soup.find_all("a", href=True):
            if "/kz/tko/control/journal/" in a["href"]:
                subject_name = a.get_text(strip=True)
                subject_links.append((subject_name, BASE_URL + a["href"]))

        if not subject_links:
            return "âš ï¸ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°. Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, cookie Ğ½ĞµĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°."

        # 2ï¸âƒ£ ĞĞ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ²ÑĞµ Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ñ‹ Ğ¸ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸
        all_grades = ["ğŸ“˜ *Ğ¢Ğ²Ğ¾Ğ¸ Ğ¶ÑƒÑ€Ğ½Ğ°Ğ»Ñ‹:*"]
        for subject, link in subject_links:
            try:
                resp = await client.get(link, headers=headers)
                if resp.status_code != 200:
                    all_grades.append(f"{subject} â€” âš ï¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° ({resp.status_code})")
                    continue

                grades = extract_grades_from_html(resp.text)
                if grades:
                    all_grades.append(f"{subject}: {grades}")
                else:
                    all_grades.append(f"{subject}: âš ï¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")

            except Exception as e:
                all_grades.append(f"{subject}: âš ï¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° ({e})")

        return "\n".join(all_grades)


def extract_grades_from_html(html: str) -> str:
    """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°"""
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table")
    if not table:
        return ""

    rows = table.find_all("tr")
    grades = []

    for row in rows[1:]:
        cols = [c.get_text(strip=True) for c in row.find_all("td")]
        if len(cols) >= 2:
            grade = cols[-1]
            if grade.isdigit() or grade in ["5", "4", "3", "2"]:
                grades.append(grade)

    return ", ".join(grades) if grades else ""
