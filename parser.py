import httpx
from bs4 import BeautifulSoup
from datetime import datetime

BASE_URL = "https://college.snation.kz"
JOURNAL_LIST_URL = f"{BASE_URL}/kz/tko/control/journals"

async def get_journal_with_cookie(cookie: str):
    """
    –ü–æ–ª—É—á–∞–µ—Ç HTML –≤—Å–µ—Ö –∂—É—Ä–Ω–∞–ª–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ cookie.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–æ—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É —Å –ø—Ä–µ–¥–º–µ—Ç–∞–º–∏ –∏ –æ—Ü–µ–Ω–∫–∞–º–∏.
    """
    now = datetime.now()
    headers = {
        "Cookie": cookie,
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/129.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Referer": JOURNAL_LIST_URL,
    }

    async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as client:
        # 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–º–µ—Ç–æ–≤
        resp = await client.get(JOURNAL_LIST_URL, headers=headers)
        if resp.status_code != 200:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–µ–¥–º–µ—Ç–æ–≤ ({resp.status_code})"

        soup = BeautifulSoup(resp.text, "html.parser")

        subject_links = []
        for a in soup.find_all("a", href=True):
            if "/kz/tko/control/journals/" in a["href"]:
                name = a.get_text(strip=True)
                journal_id = a["href"].split("/")[-1]
                if journal_id.isdigit():
                    subject_links.append((name, journal_id))

        if not subject_links:
            return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∂—É—Ä–Ω–∞–ª—ã. –ü—Ä–æ–≤–µ—Ä—å cookie."

        results = [f"üìò *–û—Ü–µ–Ω–∫–∏ –∑–∞ {now.month:02d}/{now.year}:*"]

        # 2Ô∏è‚É£ –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–º–µ—Ç–∞ –ø–∞—Ä—Å–∏–º —Ç–∞–±–ª–∏—Ü—É
        for subject, journal_id in subject_links:
            load_url = f"{JOURNAL_LIST_URL}/{journal_id}/load-table"
            params = {"year_month": f"{now.month:02d}/{now.year}"}

            headers.update({
                "Accept": "text/html, */*; q=0.01",
                "X-Requested-With": "XMLHttpRequest",
                "Referer": f"{JOURNAL_LIST_URL}/{journal_id}",
            })

            try:
                r = await client.get(load_url, headers=headers, params=params)
                if r.status_code != 200:
                    results.append(f"{subject}: ‚ö†Ô∏è –û—à–∏–±–∫–∞ {r.status_code}")
                    continue

                grades = extract_grades_from_html(r.text)
                if grades:
                    avg = round(sum(map(int, grades)) / len(grades), 1)
                    results.append(f"üìö *{subject}*: {', '.join(grades)} (—Å—Ä. {avg})")
                else:
                    results.append(f"üìö *{subject}*: ‚ö†Ô∏è –û—Ü–µ–Ω–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

            except Exception as e:
                results.append(f"{subject}: ‚ö†Ô∏è –û—à–∏–±–∫–∞ ({e})")

        return "\n".join(results)

def extract_grades_from_html(html: str):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ –∏–∑ HTML –ø–æ div.sc-journal__table--cell-value
    """
    soup = BeautifulSoup(html, "html.parser")
    divs = soup.find_all("div", class_="sc-journal__table--cell-value")
    grades = [div.get_text(strip=True) for div in divs if div.get_text(strip=True).isdigit()]
    return grades
